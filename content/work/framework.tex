% !TeX encoding = UTF-8
% !TeX spellcheck = en_US
% !TeX root = ../../../main.tex

\section{The framework}
\label{sec:framework}

The following section describes the framework which was developed during this thesis and how it could be used and configured. The framework was developed in two iterations. The first iteration was merely used during the initial experiments and resulted in a collection of \MATLAB scripts, barely useful for reuse but brought some insights into the possibilities of \MATLAB and for the next version of the framework. The second iteration was developed with having a future reuse and extendability in mind.

\subsection{Initial implementation for the basic experiments}

\begin{wrapfigure}{r}{0.4\textwidth}
\includegraphics[width=\textwidth]{content/pictures/v1_dirs.png}
\caption{Framework v1 Directory layout}
\label{fig:framework_v1_dirs}
\end{wrapfigure}

The first version of the framework has its primary focus on the experiments involving different feature algorithms and the general investigation of the basic codebook performance. The main directory for the implemented \MATLAB scripts is the \textit{common} folder. It contains 16 files which are responsible for the extraction of features, clustering with k-Means and \ac{GMM}, generation of codebooks and visualization of the codebook comparisons. These scripts share and use in total 57 functions which resides inside the \textit{common/lib} folder to accomplish their work. As the experiments in this stage were done with \ac{HOG} and \ac{WHOG} in parallel, the folders \textit{normal} and \textit{whitened} respectively contain independent settings and scripts which require a different handling based on the used feature backend. The \textit{scripts} folder contain some shell and utility scripts for setting up the environment and launching the different experiments. The \ac{VOC2011} dataset resides in the \textit{DBs} folder and the \textit{masato} folder contains some helper functions provided to me by Masato Takami. Beside of these helper functions, all other functions or libraries not written by myself are located in \textit{common/vendor}. This includes the ExemplarSVM \cite{Malisiewicz2011}, yael \cite{yael} and flann \cite{muja_flann_2009} libraries.

Depending on the chosen feature backend a folder with an optional tag (for example \textit{data\_boxed-approach} or \textit{data\_svm}) is created inside the \textit{normal} or \textit{whitened} folder by the setup scripts. This data folder is afterwards used throughout all scripts and functions as a target for the experiment results and within a \textit{cache} subfolder for caching the results. The cached data includes

\begin{itemize}
\item Features per image as
    \begin{itemize}
    \item boxed $\Rightarrow$ only the first bounding box for the current class is stored
    \item inverted boxed sliding $\Rightarrow$ everything except the bounding boxes for the current class in a sliding window manner. Used for the negative set during the \ac{SVM} training.
    \item custom $\Rightarrow$ used for the first implementation of integral images, typically the complete image
    \end{itemize}
\item Codebooks, organized by
    \begin{itemize}
    \item \dots their number of dimensions.
    \item \dots the number of parts per window.
    \item \dots the feature type as for the cached features.
    \end{itemize}
\item \acp{SVM}, organized like the codebooks with an additional separation if trained with
    \begin{itemize}
    \item \dots k-Means clusters.
    \item \dots fisher vectors.
    \end{itemize}
\end{itemize}

During the development process several utility applications were created with the python\footnote{\url{https://www.python.org}} programming language. The two main applications are \ac{GUI} based programs to work with a list of PASCAL images and group them in up to four different lists or remove some of the images from a list. The first application was written because it was required during the experiments to create multiple lists of bicycles, grouped by their orientation. As the image lists contained a high amount of images and the verification of which image is meant by a given image id was a time consuming task, the second application was written to simplify the removal of images from a given list. Both applications can be found in the git repository at \url{https://github.com/bluec0re/pascal-utils}\footnote{Revision at the time of writting: 7a4be4c941a03bc2810dd2fbbec3f336ff38fdc3}.

\subsection{Implementation of the different approaches}

After the first experiments with the feature algorithms, codebook dimensions and \ac{SVM} usage were done, the second version of the framework was developed. The framework design was orientated on the ExemplarSVM framework. This means that some of the functions are reused as for example the parsing routines for a PASCAL dataset like \ac{VOC2011}. In contrast to the first version, no global variables are used (except for caching during automated tests). Instead all settings and temporary information is stored in a configuration structure which each function expect as its first argument. Besides of some ExemplarSVM settings used for the feature extraction, all parameters which affect the execution flow are defined as fields in the structure. The default settings as described in \prettyref{tab:configuration_framework} can be obtained by calling the \verb|get_default_configuration| function. Two utility functions for providing easy-to-use wrapper functions like the \verb|demo| function are \verb|parse_keywords| which allows to parse a list of arguments in a key value like fashion. In contrast to the \verb|inputParser| class from \MATLAB, a predefinition of the available arguments is not required (although they can be restricted according to a list of strings) and the \verb|merge_structs| function which merges multiple structs into a single struct. By combining the three functions, a simple configuration by using function arguments is implemented.

Similar to first version, every function which generates information from images contain routines to save and load data from a file based cache. The whole framework is designed to provide a transparent generation of data, saving and loading it from the cache. If the user calls such a function, it initially checks if a cache file exist at a file path constructed from the chosen configuration parameters. If available, the cache file will be loaded and returned. If not, it will try to generate the requested information by using the supplied arguments.

\begin{wrapfigure}{r}{0.35\textwidth}
\includegraphics[width=\textwidth]{content/pictures/v2_full}
\caption{Filestructure of the framework}
\label{fig:v2_full}
\end{wrapfigure}

Most of the input and output values in the framework are structures or structure arrays. This provides the ability to access information on a per image base or getting an information of all images at once with a single line of code. Every time a higher amount of memory is required for arrays, its memory will be prealloctated if possible to reduce the overhead due to memory allocations and movement. For achieving this for structure arrays, a function \verb|alloc_struct_array| was implemented.

The \verb|demo| function represents a complete sample implementation using all modules from the framework. The function recognizes if it was run from a desktop or commandline (launched with the \textit{-nodesktop} option) instance of \MATLAB. If a desktop instance is used, the user will be asked interactively for a query image and afterwards to select a part from it to search for. If invoked from within a commandline instance the filename and bounding box is taken from the configuration structure.

The first step in the \verb|demo| function after a bounding box was selected is the creation of the query codebook. This is abstracted by the \verb|extract_query_codebook| function, which calculates the codebook from the image file or extracts it from the integral image for the query image. In the second step a \ac{SVM} is trained through the \verb|get_svms| function. If the calibration is enabled, the third step includes the calculation of sliding windows and corresponding codebooks for the query image and scoring them with the previously trained \ac{SVM}. A gaussian curve is afterwards fitted onto the scores and later used to adjust the scores obtained from the image database. The next step includes the loading of the corresponding database based on the size of the query part. When the database is loaded the currently computational most expensive part comes into play which calculates the sliding windows and extracts the codebooks from the integral images. For a speed up the work will be distributed over several pool workers. It had to be ensured that the data which has to be transfered between the workers and the main program is as small as possible. Tests showed that the parallization could even increase the computation time if the data required by the different workers becomes to large. Therefore it brings only a significant speed up if kd-Trees were used or the matrix reconstruction is done on the workers instead in the main program. The obtained codebooks are afterwards classified by the \ac{SVM}. If a calibration was used, all windows with scores above $-0.25$ are extracted from their source image and saved in the result folders. Without calibration, the first 100 windows after a non maximum suppression (sorted by decreasing scores) are saved. In addition a \textit{results.mat} file is also placed inside the folder. These files could be collected by the \verb|measure_results| function which calculates precision-recall curves and provide overviews about how the different parameter combinations behaved according to the ground truth from the \ac{VOC2011} dataset.


\subsubsection{Setup}

The framework uses functions from the ExemplarSVM framework\footnote{\url{https://github.com/quantombone/exemplarsvm} (visited on 11/4/2015)} \cite{Malisiewicz2011}, YAEL\footnote{\url{https://yael.gforge.inria.fr} (visited on 10/29/2015)} \cite{yael} and some smaller functions from the \MATLAB file exchange\footnote{\url{https://www.mathworks.com/matlabcentral/fileexchange/} (visited on 11/4/2015)}. Whilst YAEL and the smaller functions are included in the framework package, ExemplarSVM has to be downloaded manually and placed in the \textit{vendors} folder. The PASCAL image dataset has to be located in the folder \textit{DBs/Pascal/VOC2011}. This folder could also be adjusted in the \textit{get\_default\_configuration.m} file. An image database is defined as simple text file containing one line per image included in the database. A line is split in two columns, the first column contains a PASCAL image ID, which is typically the filename of an image without its extension (e.g. 2008\_000615), and if a PASCAL class is specified a number expressing if the image contains an object of the class (zero, no object, or one, contains object). For example, the database file \textit{bicycle\_database2.txt} contains a list of images with an indicator if the listed image contains a bicycle. Such a file is in default stored in the \textit{data} directory, but this could also be adjusted in the configuration (parameters \textit{dataset.clsimgsetpath} and \textit{dataset.imgsetpath}).

A complete usage of the framework can be found in the \textit{demo.m} file, which also allows to do a query interactively and automated with any configuration parameter adjusted as described in \prettyref{tab:configuration_framework}.

Most of the data inside the framework is represented by \MATLAB structure arrays. As they need to carry additional meta information to represent the structure, the memory usage slightly increases compared to simple matrices. But this additional information is negligible if the amount of data increases.
The benefit by using structure arrays is the possibility to access single fields of a structure array separately and also restrict the data to specific ranges. Some of the used data structures are listed in the appendix \ref{apx:data_structures}


\subsection{Modular Framework}

During the development, a more modular framework was also tested. This would enable the user to combine different, smaller blocks together to try different approaches more quickly. The different blocks would be described by configuration files which enables front end software to provide an user friendly access to them as for example in a website interface as shown in figure \ref{fig:modular_webpage}.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{images/modular_webpage}
\caption{Webinterface to the modular framework}
\label{fig:modular_webpage}
\end{figure}

As this modularity obviously requires a stricter programming interface, a throughout framework design, a lot of additional information stored aside each function and increased processing times due to the abstracted layers, this approach was not further used besides of a proof of concept implementation. The focus was instead brought back to the second framework implementation, to circumvent the restrictions coming with the modular one. Nevertheless, such a technique could be used to quickly create new algorithms by reusing existing code blocks (which have to be reusable by definition).