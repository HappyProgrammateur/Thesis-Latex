\section{Utilizing Data Structures}
\label{sec:utilizing_data_structures}

As a lot of computation time is required to extract codebooks from windows, the idea came up to reduce the amount of windows by removing the windows containing no or little data which would produce a high score. As the \ac{SVM} training is done before the codebook extraction takes place, we can utilize the information which we have got from the training. The \ac{SVM} estimated a weight for every codebook dimension, expressing the importance for an equality to the query part. This weights can be used to early remove the all windows which would probably never get a high equality score.

The most simplest approach uses the integral image representation to mark potential areas in the target. The na\"{i}ve version implemented considers each pixel position as a potential candidate if it contains at least one dimension which got a weight above the mean of all positive weights ($w_j > \frac{sumOfWeightsAboveZero}{numberOfWeightsAboveZero}$). As one may notice, this would only remove windows at the very top left area of the integral image, as each occurrence is carried with in the bottom right direction due to the nature of an integral image. Nevertheless, the amount of windows can be reduced with this approach and false negative rate was not increased as it never removed any important parts of an image due to the conservative methodology.
\bigskip

Based on the data structures which were used during the memory optimization, further optimizations can be done. As most of the techniques to reduce the memory overhead require to know the position were a new feature was found, this information can also be used to reduce the amount of windows which have to be scored by the \ac{SVM}. The idea is to remove all windows which contain none of the most important codebook dimensions based on the \ac{SVM} weights. In the kd-Tree storage, one tree per dimension exist and therefore each window can be easily searched for specific dimensions without rebuilding a complete codebook. The current implementation stores two types of trees to optimize this window filter. The original trees which also contain the points which exist at the cross sections of features (required to fully reconstruct a codebook in an integral image). And a stripped down version which contains only the points of the integral image were a feature was added. The stripped trees are used to find the windows which contain entries at a required codebook dimension, whereas the extended trees are used to reconstruct a codebook for each of the remaining windows.
\bigskip

A second idea was to completely omit the generic window list generation and instead guessing the candidate windows from the list of feature points. This could be done by voting for different, rectangle windows around each feature point. If a specific window gains enough votes from its enclosed area, it would be considered as a potential part candidate. This should reduce the amount of candidates even further and additional increase the detection rate as areas with wrong or to few information would not be considered at all. Nevertheless, a higher false negative rate is possible as some areas actually containing the searched part may do not contain enough of the dimensions considered important for the query part. The ratio between the false negative rate and the computational effort could be adjusted by using different thresholds for selecting representative dimensions. Due to time limitations, this potential approach could not be implemented and evaluated.