\chapter{Current Approaches}

The area of image part retrieval based on the content is a field which is formulated since the 1990s \cite{eakins1999content} \cite{rui1997content} \cite{osuna1997training}. As the general field of image classification is focused on algorithms to provide the most accurate classification, the part retrieval also has a higher dependency on the processing time. This circumstance is based on the problem that current segmentation and grouping techniques provide not as good results as compared to the classification of multiple image regions in different scales and ratios \cite{book:848523}. As the processing speed becomes more important if more regions are searched, former approaches used the color distribution to define similarities to decrease the required time to search. The structural and texture information come into play as more computation power becomes available and allow to use more expensive techniques. Many approaches were designed for a specific task to get the optimum ratio between processing time and query results. One of the examples is the face detection algorithm based on Haar features and boosting cascades by Viola and Jones \cite{viola2001rapid}. Current approaches are mostly based on algorithms describing feature already used in content based image classification tasks. Some of the mostly used algorithms are \acf{SIFT} \cite{Lowe2004}, \acf{HOG} \cite{Dalal2005} and \acf{SURF} \cite{bay2008speeded}. As those feature descriptors already produced reasonable results for searching a particular object with classification methods like \aclp{SVM} \cite{cortes1995support}, the research goes into building up visual categories to transfer meta information between the different detections.

One major algorithm is the ExemplarSVM \cite{Malisiewicz2011}. It provided an approach to find objects contained in images not only based on the query objects type, but also to categorize them into different orientations and shapes.
It is based on the \ac{HOG}\footnote{see \prettyref{sec:hog} for a more detailed explanation} feature descriptors and \ac{SVM}\footnote{see \prettyref{sec:svm} for a more detailed explanation} for the classification. As the training of an appropriate \ac{SVM} for an object category requires a large set of negative samples to be used together with multiple positive samples to get an adequate generic classifier, Malisiewicz et al. use the approach to train multiple \acp{SVM} with less samples. Each of this classifiers are trained with only one positive sample and multiple negative samples. This allowed them to use one \ac{SVM} only for one sample to get visually similar objects instead of a complete category. As this requires a \ac{SVM} algorithm which only has to solve this simpler task and can compete against an over-fitting based on the single positive sample.

During the \ac{SVM} training, they use a mining approach as described by Felzenszwalb et al. \cite{felzenszwalb2010object} to find hard negative examples for the training. This is done by doing multiple training rounds with additional, reoccurring tests of the trained \ac{SVM}. During each round of the training, false negatives of the tests are added to the negative set of the next training round. This allows the \ac{SVM} to adjust its weights based on the classification results it produces.

As multiple \acp{SVM} are trained, the output scores do not necessarily match together. To fix this issue, the authors additionally apply a calibration. This is done by scoring the \ac{SVM} proposals with the PASCAL overlap score (50\% or more overlap with the ground truth is considered as positive) and fitting a logistic function to it. The function is used to move the decision boundaries of the different exemplars. For poorly performing exemplars, the boundary is moved towards them and for well-performing ones away from them. The function to adjust the score of a detection $x$ with the function parameters $\alpha_E$ and $\beta_E$ is shown in \eqnref{esvm:calibration}.

\begin{equation}
f(x|w_E, \alpha_E, \beta_E) = \frac{1}{1 + e^{-\alpha_E (w^T_E x - \beta_E)} }
\label{eqn:esvm:calibration}
\end{equation}

By combining the different approaches, the ExemplarSVM algorithm is able to reach a mean average precision (\acsu{mAP}; detailed description at \ref{sec:basic:terminology}) of 0.4 with the bicycle class of the \ac{VOC2007} dataset \cite{Pascal2007} and a 0.19 \ac{mAP} on all classes. Compared to a \ac{NN} approach, this is a 2 to 3 times better performance (0.056 and 0.11 respectively).