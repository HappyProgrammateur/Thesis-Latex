\chapter{Conclusion}

The experimental results showed that the total query time can be reduced to a reasonable low amount, even if the queried database actually represents codebooks at every pixel with several hundred entries. Nevertheless, one of the bases of the algorithm (the codebooks) does not seem to provide the desired performances. One of the reasons may be the loss of the information about the exact locations and the sizes of the input feature patches. Another reason could be that the reduction of the 775 dimensional features to a codebook shadows the essential information which actually represents the class of the query image. As the current used system is unsupervised at the database creation, no information is available for the feature extraction nor which feature patch sizes should be included into a codebook. Furthermore, some of the experimental results indicate that smaller image parts with lesser feature patches getting lost. This is based on the fact that a higher amount of features which add up would produce a higher profile codebook compared to a lower amount of features. Approaches like ExemplarSVM which operate directly on the feature descriptors have to define the similarity between the descriptors themselves, whilst the presented approach has a higher dependency on the sorting of the describtors into the codebook bins. This means a sort of similarity assignments is required to preserve the performance of approaches which does not use this additional layer.

The presented approach contained some promising parts (for example the ability to precompute a whole image database and extract arbitrary codebooks at query time), but can't compete against algorithms like ExemplarSVM. Nevertheless, additional optimizations in terms of program code structure could reduce the query time even further, and therefore could be used as a filter algorithm to reduce the effort for more accurate algorithms.

\chapter{Outlook}

Next generations of the framework could get further speed and memory usage improvements by using code optimizations as the current framework is developed to compare different approaches and replace different parts. Also further experiments with the codebook approach could increase the detection performance. Possible improvements could be done in the separation of the input patches based on their sizes, also different feature pyramids could be created to get a broader range of input patches which get combined into the database. The current method of constructing the codebook entries could be changed to allow fuzzy assignments and using a normalization to get higher profiles even if only a small amount of patches are available. Further improvements to the loading and accessing of the database entries could be done by introducing hash tables instead or with the kd-Tree model, which would speed up the accessing time even more.

Another approach, which tries to go in the opposite direction, would be the usage of the integral image to estimate the existence of a searched object by triangulating it similar to the currently used prefilter. This could maybe replace the sliding window approach and therefore decrease the effort dramatically.

Improvements compared to other algorithms based on hardware improvements would possibly only arise in terms of memory and file space, as the integral images could be used without the need of complicated data structures and individual entries could therefore be accessed in linear time.